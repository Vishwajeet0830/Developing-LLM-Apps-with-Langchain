{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f80e2d6-5b1e-4351-a3b0-40e2960ad04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0831a08-5cf9-4a9e-a24a-139e13f6dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.2.16\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: C:\\Users\\Jeet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-community\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34596291-09de-44bc-b494-20f901ab5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.44.1\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: C:\\Users\\Jeet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75aace9-7dff-4ca8-b001-88b4e3715925",
   "metadata": {},
   "source": [
    "### Python-Dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cd211f-0175-4d9b-9a4c-6af0e75a85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Check if environment variable is loaded\n",
    "print(os.getenv('OpenAI_API_Key'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf82127-8b0b-4eaf-87ef-e04e204fa312",
   "metadata": {},
   "source": [
    "### Chat Models - GPT-3.5 Turbo and GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cff4158-7b96-48dd-9de7-58eeea95a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeet\\AppData\\Local\\Temp\\ipykernel_26784\\3743972456.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # You can also use \"gpt-4\" for GPT-4\n",
      "C:\\Users\\Jeet\\AppData\\Local\\Temp\\ipykernel_26784\\3743972456.py:7: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  output = llm.predict(\"Explain quantum mechanics in one sentence.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of matter and energy at the smallest scales, where particles exhibit both wave-like and particle-like properties.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI chat model (make sure your OpenAI API key is set)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # You can also use \"gpt-4\" for GPT-4\n",
    "\n",
    "# Invoke the LLM with a prompt\n",
    "output = llm.predict(\"Explain quantum mechanics in one sentence.\")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b47128-741c-4802-9127-53e3032fff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf sehr kleinen Skalen und ermöglicht Vorhersagen über ihre Eigenschaften mit Hilfe von Wahrscheinlichkeitsverteilungen.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain Quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053f663-e025-45b0-9a37-84c960acc96c",
   "metadata": {},
   "source": [
    "### Caching LLM Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ea95d-8a40-4b72-beef-106a7e3cec28",
   "metadata": {},
   "source": [
    "### In-memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f45a216-20fe-4eb0-9f8f-afb10c2c8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name = 'gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf819c57-a002-46dd-8aab-f9d6b724b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.39 s\n",
      "Wall time: 15.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a827b5-d175-4f5f-ad72-c01a44d20547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad?\\n\\nBecause it had too many problems!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f977358f-0058-4c34-bf30-df4942a924cc",
   "metadata": {},
   "source": [
    "### SQLITE Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55c6596-cc15-4c34-a550-3c8e359d99f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "#first request not in cache takes longer\n",
    "llm.invoke('Tell me a joke')\n",
    "\n",
    "#second request in cache is faster\n",
    "llm.invoke('Tell me a joke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac2597-1029-4e4d-8e45-879a6d01e218",
   "metadata": {},
   "source": [
    "### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c2c5b9-41d0-4ad3-b6e6-87e718d5a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Beneath the silver moonlight\n",
      "The raven takes flight\n",
      "Singing a haunting tune\n",
      "As the night comes alive\n",
      "\n",
      "Chorus:\n",
      "Oh moon, oh raven\n",
      "Guide me through the night\n",
      "With your eerie glow\n",
      "And your chilling cries\n",
      "\n",
      "Verse 2:\n",
      "The raven perched high\n",
      "On a branch so black\n",
      "Watching over the land\n",
      "With eyes so sharp and keen\n",
      "\n",
      "Chorus:\n",
      "Oh moon, oh raven\n",
      "Guide me through the night\n",
      "With your eerie glow\n",
      "And your chilling cries\n",
      "\n",
      "Bridge:\n",
      "In the darkness we find solace\n",
      "In the shadows we find peace\n",
      "The moon and the raven\n",
      "Our guardians in the night\n",
      "\n",
      "Chorus:\n",
      "Oh moon, oh raven\n",
      "Guide me through the night\n",
      "With your eerie glow\n",
      "And your chilling cries\n",
      "\n",
      "Outro:\n",
      "As the night fades away\n",
      "And the sun begins to rise\n",
      "We thank the moon and the raven\n",
      "For watching over us tonight.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the moon and the raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ce56db-086c-42fd-be80-4725843983be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "The moon shines bright in the midnight sky\n",
      "Casting shadows as the raven flies\n",
      "Through the darkness, it calls my name\n",
      "A haunting melody, driving me insane\n",
      "\n",
      "(Chorus)\n",
      "Oh, moon and raven, guide me through the night\n",
      "Illuminate my path, show me the light\n",
      "Together we'll soar, free from the chains\n",
      "Moon and raven, forever we'll reign\n",
      "\n",
      "(Verse 2)\n",
      "The raven's wings beat against the wind\n",
      "A symphony of darkness, a dance of sin\n",
      "It whispers secrets in the dead of night\n",
      "A silent companion in the fading light\n",
      "\n",
      "(Chorus)\n",
      "Oh, moon and raven, guide me through the night\n",
      "Illuminate my path, show me the light\n",
      "Together we'll soar, free from the chains\n",
      "Moon and raven, forever we'll reign\n",
      "\n",
      "(Bridge)\n",
      "In the stillness of the night, we'll find our way\n",
      "Beneath the silver glow, we'll never stray\n",
      "The moon and the raven, a bond so strong\n",
      "Together we'll conquer, forever we'll belong\n",
      "\n",
      "(Chorus)\n",
      "Oh, moon and raven, guide me through the night\n",
      "Illuminate my path, show me the light\n",
      "Together we'll soar, free from the chains\n",
      "Moon and raven, forever we'll reign\n",
      "\n",
      "(Outro)\n",
      "Moon and raven, a timeless pair\n",
      "In the darkness, we'll always share\n",
      "Through the shadows, we'll never part\n",
      "Moon and raven, forever in my heart."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23e166-17c2-4848-9d1f-86e81b5dc075",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbcbae2c-4a08-43f5-97c4-1931d3190519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced viralogist.\\nWrite a few sentences about the following virus \"hiv\" in german.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "template = '''You are an experienced viralogist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(virus='hiv', language = 'german')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f4e5c75-3d29-4682-8a11-f758bb7f9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV, das humane Immundefizienzvirus, ist ein Virus, das das Immunsystem des Menschen schwächt und zu AIDS führen kann. Es wird hauptsächlich durch ungeschützten Geschlechtsverkehr, den Austausch von infizierten Nadeln oder von der Mutter auf das Kind während der Schwangerschaft übertragen. Es ist wichtig, sich über die Übertragungswege und Präventionsmaßnahmen von HIV zu informieren, um die Verbreitung dieser lebensbedrohlichen Krankheit einzudämmen.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27913fa4-bf41-4157-8889-54c81a7f2472",
   "metadata": {},
   "source": [
    "### ChatPrompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef87bcf7-dfed-4ace-81dc-1e2ed066b044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in JSON fromat.'), HumanMessage(content='Top 10 countries in europe by population.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in JSON fromat.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='10', area='europe')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e98576-53d2-4b7b-85c1-3ec6e5504ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\"rank\": 1, \"name\": \"Germany\", \"population\": 83149300},\n",
      "        {\"rank\": 2, \"name\": \"France\", \"population\": 67146000},\n",
      "        {\"rank\": 3, \"name\": \"United Kingdom\", \"population\": 66040229},\n",
      "        {\"rank\": 4, \"name\": \"Italy\", \"population\": 60589445},\n",
      "        {\"rank\": 5, \"name\": \"Spain\", \"population\": 46441049},\n",
      "        {\"rank\": 6, \"name\": \"Ukraine\", \"population\": 43733762},\n",
      "        {\"rank\": 7, \"name\": \"Poland\", \"population\": 38433600},\n",
      "        {\"rank\": 8, \"name\": \"Romania\", \"population\": 19405156},\n",
      "        {\"rank\": 9, \"name\": \"Netherlands\", \"population\": 17424978},\n",
      "        {\"rank\": 10, \"name\": \"Belgium\", \"population\": 11484055}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "694b6763-dddf-4681-8dcb-907e51406779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in JSON fromat.'), HumanMessage(content='Top 5 countries in asia by population.')]\n"
     ]
    }
   ],
   "source": [
    "messages = chat_template.format_messages(n='5', area='asia')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bc95f65-671e-4b2e-ae8b-067aa7cad603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\"country\": \"China\", \"population\": 1439323776},\n",
      "        {\"country\": \"India\", \"population\": 1380004385},\n",
      "        {\"country\": \"Indonesia\", \"population\": 273523615},\n",
      "        {\"country\": \"Pakistan\", \"population\": 220892331},\n",
      "        {\"country\": \"Bangladesh\", \"population\": 164689383}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedb55b-b972-4858-be07-2121f620e971",
   "metadata": {},
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "199255e4-56ed-47ed-9459-f81bc2489714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are experienced virologist. Write a few sentences about the following virus\"HSV\" in Spanish.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'HSV', 'language': 'Spanish', 'text': 'El virus del herpes simple (HSV) es un virus de doble cadena de ADN que pertenece a la familia de los herpesvirus. Hay dos tipos principales de HSV: HSV-1, que generalmente causa herpes labial, y HSV-2, que generalmente causa herpes genital. El HSV es altamente contagioso y se transmite a través del contacto directo con las lesiones activas. Una vez infectado, el virus permanece en el cuerpo de por vida y puede causar brotes recurrentes de síntomas.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are experienced virologist. Write a few sentences about the following virus\"{virus}\" in {language}.'''\n",
    "prompt_template =PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt_template,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus':'HSV','language':'Spanish'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39f260de-7c31-41fb-b6f8-e72f847dfd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Country:  India\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'virus'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m chain\u001b[38;5;241m=\u001b[39mLLMChain(\n\u001b[0;32m      4\u001b[0m     llm \u001b[38;5;241m=\u001b[39m llm,\n\u001b[0;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_template,\n\u001b[0;32m      6\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m country \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter Country: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:152\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    146\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m     inputs,\n\u001b[0;32m    148\u001b[0m     run_id,\n\u001b[0;32m    149\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\chains\\base.py:282\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    280\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing some input keys: {'virus'}"
     ]
    }
   ],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use Bullet Points'\n",
    "prompt_templae = PromptTemplate.from_template(template=template)\n",
    "chain=LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt_template,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccc3f5-bb04-41df-acef-76144295b176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
